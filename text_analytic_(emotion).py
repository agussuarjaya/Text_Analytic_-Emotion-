# -*- coding: utf-8 -*-
"""Text Analytic (Emotion).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OJWy2VGhERdH8uws0R47Y6SamY5JCo8k
"""

"""

  Text Analytic (Emotion) with TensorFlow
 
  Copyright 2019  I Made Agus Dwi Suarjaya
                  Author 2
                  Author 3

  Description     : Try to analyze Tweets with TensorFlow and classify into 5 emotions (anger, happiness, sadness, love, fear)
  Dataset source  : https://raw.githubusercontent.com/meisaputri21/Indonesian-Twitter-Emotion-Dataset/master/Twitter_Emotion_Dataset.csv

"""

#Setup
import tensorflow.compat.v2 as tf
import tensorflow_datasets as tfds

import csv
import time
import ast

import numpy as np
import pandas as pd

#Enable eager for easy experiment
tf.compat.v1.enable_eager_execution()

#--------------------------------------------------------------------------------------------------------------------------
"""
#Get dataset by url into panda dataframe
dataset_url = 'https://raw.githubusercontent.com/meisaputri21/Indonesian-Twitter-Emotion-Dataset/master/Twitter_Emotion_Dataset.csv'
df1 = pd.read_csv(dataset_url)
#print (df1)                                                #Take a look at the dataframe

#Make category dictionary for the label
c = df1.label.astype('category')
d = dict(enumerate(c.cat.categories))
#print (d)                                                  #Take a look at the dictionary

#Convert categories label to codes
df1['label'] = pd.Categorical(df1['label'])
df1['label'] = df1.label.cat.codes

#df1.head()                                                 #Take a look at the head of dataframe

#Split then combine dataframe into tensorflow dataset
target = df1.label
featu = df1.tweet

dataset = tf.data.Dataset.from_tensor_slices((featu.values, target.values))

#for feat, targ in dataset.take(5):
#  print ('Features: {}, Target: {}'.format(feat, targ))    #Take a look at the dataset

#Tokenize the strings into tokens then make vocabulary set of the dataset
tokenizer = tfds.features.text.Tokenizer()

vocabulary_set = set()
for text_tensor, _ in dataset:
  some_tokens = tokenizer.tokenize(text_tensor.numpy())
  vocabulary_set.update(some_tokens)

vocab_size = len(vocabulary_set)
#vocab_size                                                 #Take a look at the vocabulary size
#vocabulary_set                                             #Take a look at the content of vocabulary set

encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)

#For testing the encoder with example text
#example_text = next(iter(dataset))[0].numpy()
#print(example_text)
#encoded_example = encoder.encode(example_text)
#print(encoded_example)

#Function to encode text into tokenized text
def encode(text_tensor, label):
  encoded_text = encoder.encode(text_tensor.numpy())
  return encoded_text, label

#Function to encode and map dataset into encoded dataset
def encode_map_fn(text, label):
  encoded_text, label = tf.py_function(encode, inp=[text, tf.cast(label, tf.int64)], Tout=(tf.int64, tf.int64))

  encoded_text.set_shape([None])
  label.set_shape([])

  return encoded_text, label


encoded_dataset = dataset.map(encode_map_fn)

#for ex in encoded_dataset.take(5):
#  print(ex)                                              #Take a look at the encoded dataset

#Shuffle the dataset
encoded_dataset = encoded_dataset.shuffle(50000, reshuffle_each_iteration=False)

#Split dataset into train dan test dataset
train_dataset = encoded_dataset.take(4000)
test_dataset = encoded_dataset.skip(4000)

#Padd the dataset to have the same length
train_dataset = train_dataset.padded_batch(40, 
                                           padded_shapes=([None],[]))
test_dataset = test_dataset.padded_batch(40, 
                                         padded_shapes=([None],[]))

#Take a look at the padded dataset
#sample_text, sample_labels = next(iter(test_dataset))
#
#for i in range(10):
#  print(sample_text[i], sample_labels[i])

#Make the model and set the hyperparameters
model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(vocab_size + 1, 64))            #Add 1 to vocabulary size because of padding
model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))

for units in [64,16]:
  model.add(tf.keras.layers.Dense(units, activation='relu'))

model.add(tf.keras.layers.Dense(5))                                 #Output layer. The first argument is the number of labels.

model.compile(optimizer='adam', 
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), 
              metrics=['accuracy'])

#Set the hyperparameters and train the model
model.fit(train_dataset, epochs=10, validation_data=test_dataset)

#Evaluate the model
eval_loss, eval_acc = model.evaluate(test_dataset)

print('\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))

#Save the model (Optional for Transfer Learning)
t = time.time()

export_path_model = "./{}_model".format(int(t))
model.save(export_path_model, save_format='h5')

#Save the encoder (Optional for Transfer Learning)
export_path_encoder = "./{}_encoder".format(int(t))
encoder.save_to_file(export_path_encoder)

#Save the dictionary (Optional for Transfer Learning)
export_path_dict = "./{}_dict".format(int(t))
with open(export_path_dict, 'w') as outfile:
    outfile.write(str(d))
"""
#--------------------------------------------------------------------------------------------------------------------------

#Load the model (Optional for Transfer Learning)
reloaded_model = tf.keras.models.load_model('./1585123477_model')
model = reloaded_model

#Load the encoder (Optional for Transfer Learning)
encoder = tfds.features.text.TokenTextEncoder.load_from_file('./1585123477_encoder')

#Load the dictionary (Optional for Transfer Learning)
with open('./1585123477_dict') as dict_file:
    d = ast.literal_eval(dict_file.readline())

#Classify some tweets with model predict
tweet = []
tweet.append('Tahukah kamu, bahwa saat itu papa memejamkan matanya dan menahan gejolak dan batinnya. Bahwa papa sangat ingin mengikuti keinginanmu tapu lagi-lagi dia HARUS menjagamu?')
tweet.append('[Idm] My, masa gua tadi ketemu tmn SD yg pas SD ngejar gua dan ngasih surat tiap minggunya, asdfghjkl bgt, gk tau knp ngerasa takut gua :v hadeuh jaman SD ngerti apa coba :v')
tweet.append('Sedih bny penulisan resep yg tidak baku sdm, sdt, ruas, sejumput, secukupnya, even biji/buah termasuk tidak baku :(')
tweet.append('Paling nyampah org suka compare kan aku dgn org lain, dia dia ah aku aku ah. Tak suka boleh blah lah')
tweet.append('Agak telat ramai nya ya dok...sudah paham sejak lama banget jadi geli aja baru pada ramai sekarang hehehe...')

for text in range(len(tweet)):
  predictions = model.predict(encoder.encode(tweet[text]))
  predictions[0]
  print(d[np.argmax(predictions[0])], ' <- ', tweet[text])
