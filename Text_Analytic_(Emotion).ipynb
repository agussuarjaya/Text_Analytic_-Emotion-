{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Analytic (Emotion).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv2g_iX_ZhvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "  Text Analytic (Emotion) with TensorFlow\n",
        " \n",
        "  Copyright 2020  I Made Agus Dwi Suarjaya\n",
        "                  Ni Luh Putu Diah Putri M\n",
        "                  Gede Ocha Dipa Ananda\n",
        "\n",
        "  Description     : Try to analyze Tweets with TensorFlow and classify into 5 emotions (anger, happiness, sadness, love, fear)\n",
        "  Dataset source  : https://raw.githubusercontent.com/meisaputri21/Indonesian-Twitter-Emotion-Dataset/master/Twitter_Emotion_Dataset.csv\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#Setup\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import csv\n",
        "import time\n",
        "import ast\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Enable eager for easy experiment\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug2EgJgLZo5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "#Get dataset by url into panda dataframe\n",
        "dataset_url = 'https://raw.githubusercontent.com/meisaputri21/Indonesian-Twitter-Emotion-Dataset/master/Twitter_Emotion_Dataset.csv'\n",
        "df1 = pd.read_csv(dataset_url)\n",
        "#print (df1)                                                #Take a look at the dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DERK4uyNZpx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Make category dictionary for the label\n",
        "c = df1.label.astype('category')\n",
        "d = dict(enumerate(c.cat.categories))\n",
        "#print (d)                                                  #Take a look at the dictionary                                           "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W23uV-HNjmOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert categories label to codes\n",
        "df1['label'] = pd.Categorical(df1['label'])\n",
        "df1['label'] = df1.label.cat.codes\n",
        "\n",
        "#df1.head()                                                 #Take a look at the head of dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3VI5CgOZwZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split then combine dataframe into tensorflow dataset\n",
        "target = df1.label\n",
        "featu = df1.tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruib85zL5O5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((featu.values, target.values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY-MiaNL22Jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for feat, targ in dataset.take(5):\n",
        "#  print ('Features: {}, Target: {}'.format(feat, targ))    #Take a look at the dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJJVJ0gH4RVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenize the strings into tokens then make vocabulary set of the dataset\n",
        "tokenizer = tfds.features.text.Tokenizer()\n",
        "\n",
        "vocabulary_set = set()\n",
        "for text_tensor, _ in dataset:\n",
        "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
        "  vocabulary_set.update(some_tokens)\n",
        "\n",
        "vocab_size = len(vocabulary_set)\n",
        "#vocab_size                                                 #Take a look at the vocabulary size\n",
        "#vocabulary_set                                             #Take a look at the content of vocabulary set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBplm2Cv4QCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fBe2TzFACCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For testing the encoder with example text\n",
        "#example_text = next(iter(dataset))[0].numpy()\n",
        "#print(example_text)\n",
        "#encoded_example = encoder.encode(example_text)\n",
        "#print(encoded_example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiLjLIG4xZbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to encode text into tokenized text\n",
        "def encode(text_tensor, label):\n",
        "  encoded_text = encoder.encode(text_tensor.numpy())\n",
        "  return encoded_text, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJF0vZaWxbhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to encode and map dataset into encoded dataset\n",
        "def encode_map_fn(text, label):\n",
        "  encoded_text, label = tf.py_function(encode, inp=[text, tf.cast(label, tf.int64)], Tout=(tf.int64, tf.int64))\n",
        "\n",
        "  encoded_text.set_shape([None])\n",
        "  label.set_shape([])\n",
        "\n",
        "  return encoded_text, label\n",
        "\n",
        "\n",
        "encoded_dataset = dataset.map(encode_map_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOv9gGRS0hzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for ex in encoded_dataset.take(5):\n",
        "#  print(ex)                                              #Take a look at the encoded dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12xAiesvY8rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Shuffle the dataset\n",
        "encoded_dataset = encoded_dataset.shuffle(50000, reshuffle_each_iteration=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o01DHStFaBI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split dataset into train dan test dataset\n",
        "train_dataset = encoded_dataset.take(4000)\n",
        "test_dataset = encoded_dataset.skip(4000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWNm9bOTWqvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Padd the dataset to have the same length\n",
        "train_dataset = train_dataset.padded_batch(40, \n",
        "                                           padded_shapes=([None],[]))\n",
        "test_dataset = test_dataset.padded_batch(40, \n",
        "                                         padded_shapes=([None],[]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME73zNcKXle7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Take a look at the padded dataset\n",
        "#sample_text, sample_labels = next(iter(test_dataset))\n",
        "#\n",
        "#for i in range(10):\n",
        "#  print(sample_text[i], sample_labels[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON1GCPy8Zy1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Make the model and set the hyperparameters\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size + 1, 64))            #Add 1 to vocabulary size because of padding\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
        "\n",
        "for units in [64,16]:\n",
        "  model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(5))                                 #Output layer. The first argument is the number of labels.\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK7hJP4eY1i0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "83d0afae-8c2d-45c3-d896-286efa645846"
      },
      "source": [
        "#Set the hyperparameters and train the model\n",
        "model.fit(train_dataset, epochs=10, validation_data=test_dataset)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "100/100 [==============================] - 14s 140ms/step - loss: 1.5344 - acc: 0.2770 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 11s 106ms/step - loss: 1.0991 - acc: 0.5297 - val_loss: 1.3864 - val_acc: 0.3840\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 11s 106ms/step - loss: 0.7881 - acc: 0.6842 - val_loss: 1.6675 - val_acc: 0.3840\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 11s 106ms/step - loss: 0.7010 - acc: 0.7055 - val_loss: 1.5811 - val_acc: 0.4564\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 11s 107ms/step - loss: 0.4939 - acc: 0.8207 - val_loss: 1.9851 - val_acc: 0.4364\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 11s 110ms/step - loss: 0.3179 - acc: 0.8953 - val_loss: 2.1108 - val_acc: 0.4738\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 11s 112ms/step - loss: 0.2938 - acc: 0.9022 - val_loss: 1.9635 - val_acc: 0.4913\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 11s 111ms/step - loss: 0.2064 - acc: 0.9295 - val_loss: 1.9000 - val_acc: 0.5436\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 11s 112ms/step - loss: 0.0815 - acc: 0.9800 - val_loss: 1.9839 - val_acc: 0.5411\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 11s 112ms/step - loss: 0.0327 - acc: 0.9933 - val_loss: 2.0343 - val_acc: 0.5586\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f538819a7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4hxHrK-Y5Gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluate the model\n",
        "eval_loss, eval_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kngId--WT3hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the model (Optional for Transfer Learning)\n",
        "t = time.time()\n",
        "\n",
        "export_path_model = \"./{}_model\".format(int(t))\n",
        "model.save(export_path_model, save_format='h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5wxZRc3O5kk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the encoder (Optional for Transfer Learning)\n",
        "export_path_encoder = \"./{}_encoder\".format(int(t))\n",
        "encoder.save_to_file(export_path_encoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDxIX3oVtsZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the dictionary (Optional for Transfer Learning)\n",
        "export_path_dict = \"./{}_dict\".format(int(t))\n",
        "with open(export_path_dict, 'w') as outfile:\n",
        "    outfile.write(str(d))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxOrVVZHUWQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "#Load the model (Optional for Transfer Learning)\n",
        "reloaded_model = tf.keras.models.load_model(export_path_model)\n",
        "model = reloaded_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAz6oTstPybv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load the encoder (Optional for Transfer Learning)\n",
        "encoder = tfds.features.text.TokenTextEncoder.load_from_file(export_path_encoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMpMXu6Xv7Bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load the dictionary (Optional for Transfer Learning)\n",
        "with open(export_path_dict) as dict_file:\n",
        "    d = ast.literal_eval(dict_file.readline())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXjS6XyS0dq-",
        "colab_type": "code",
        "outputId": "70e67400-23b0-4d8c-ea50-05bfa47cad19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "#Classify some tweets with model predict\n",
        "tweet = []\n",
        "tweet.append('Tahukah kamu, bahwa saat itu papa memejamkan matanya dan menahan gejolak dan batinnya. Bahwa papa sangat ingin mengikuti keinginanmu tapu lagi-lagi dia HARUS menjagamu?')\n",
        "tweet.append('[Idm] My, masa gua tadi ketemu tmn SD yg pas SD ngejar gua dan ngasih surat tiap minggunya, asdfghjkl bgt, gk tau knp ngerasa takut gua :v hadeuh jaman SD ngerti apa coba :v')\n",
        "tweet.append('Sedih bny penulisan resep yg tidak baku sdm, sdt, ruas, sejumput, secukupnya, even biji/buah termasuk tidak baku :(')\n",
        "tweet.append('Paling nyampah org suka compare kan aku dgn org lain, dia dia ah aku aku ah. Tak suka boleh blah lah')\n",
        "tweet.append('Agak telat ramai nya ya dok...sudah paham sejak lama banget jadi geli aja baru pada ramai sekarang hehehe...')\n",
        "\n",
        "for text in range(len(tweet)):\n",
        "  predictions = model.predict(encoder.encode(tweet[text]))\n",
        "  predictions[0]\n",
        "  print(d[np.argmax(predictions[0])], ' <- ', tweet[text])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "love  <-  Tahukah kamu, bahwa saat itu papa memejamkan matanya dan menahan gejolak dan batinnya. Bahwa papa sangat ingin mengikuti keinginanmu tapu lagi-lagi dia HARUS menjagamu?\n",
            "fear  <-  [Idm] My, masa gua tadi ketemu tmn SD yg pas SD ngejar gua dan ngasih surat tiap minggunya, asdfghjkl bgt, gk tau knp ngerasa takut gua :v hadeuh jaman SD ngerti apa coba :v\n",
            "sadness  <-  Sedih bny penulisan resep yg tidak baku sdm, sdt, ruas, sejumput, secukupnya, even biji/buah termasuk tidak baku :(\n",
            "anger  <-  Paling nyampah org suka compare kan aku dgn org lain, dia dia ah aku aku ah. Tak suka boleh blah lah\n",
            "happy  <-  Agak telat ramai nya ya dok...sudah paham sejak lama banget jadi geli aja baru pada ramai sekarang hehehe...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}